# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X3v7-9g0TwsGYvL1yYPrQp0AldYx8535

# Student ID: 2201195

**Install the required libraries**
"""

!pip install nltk

!pip install keras

!pip install transformers

!pip install torch
!pip install sklearn
!pip install tensorflow

"""**Import all the required libraries**"""

import numpy as np
import os
import pandas as pd
import torch

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix
from sklearn.preprocessing import MaxAbsScaler

import tensorflow as tf
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report

from keras.layers import Bidirectional, BatchNormalization

import pickle 
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

nltk.download('punkt')
nltk.download('stopwords')

"""**Student id to be used for seed**"""

student_id = 2201195

"""Seed libraries with student id"""

# set same seeds for all libraries

# numpy seed
np.random.seed(student_id)

# torch seed
torch.manual_seed(student_id)

"""# Common Codes 

In this section you will write all common codes, for examples


*   Data read
*   Data Splitting
*   Performance Matrics
*   Print Dataset Statistics
*   Saving model and output
*   Loading Model and output
*   etc

**Allow the GDrive access and set data and model paths**
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# initialize GDrive and data and models paths
GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807/Assignment2/',str(student_id))
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

"""**Split dataset into 4 sub datasets with different sizes of 25%, 50%, 75% and 100% with seeding the data by student id**"""

# Load the train dataset
train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv') # This is 100% of data
train_data = pd.read_csv(train_file)

# Shuffle the dataset with a fixed seed
# train_data = train_data.sample(frac=1, random_state=student_id).reset_index(drop=True)

# Split the dataset into subsets
train_data_1, remaining_data = train_test_split(train_data, test_size=0.75, stratify=train_data['label'], random_state=student_id)
train_data_2, remaining_data = train_test_split(remaining_data, test_size=0.6667, stratify=remaining_data['label'], random_state=student_id)
train_data_3, train_data_4 = train_test_split(remaining_data, test_size=0.5, stratify=remaining_data['label'], random_state=student_id)

# Combine the subsets to create different data sizes
train_data_25 = train_data_1.copy()
train_data_50 = pd.concat([train_data_1, train_data_2], ignore_index=True)
train_data_75 = pd.concat([train_data_1, train_data_2, train_data_3], ignore_index=True)
train_data_100 = pd.concat([train_data_1, train_data_2, train_data_3, train_data_4], ignore_index=True)

# Save the datasets to Google Drive
train_data_25.to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv'), index=False)
train_data_50.to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_50.csv'), index=False)
train_data_75.to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_75.csv'), index=False)
train_data_100.to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_100.csv'), index=False)

"""Read the requried files`train` `validation` and `test` files"""

train_100_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_100.csv')
train_75_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_75.csv')
train_50_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_50.csv')
train_25_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv')

val_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')
test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')

print('Train 100% file: ', train_100_file)
print('Train 75% file: ', train_75_file)
print('Train 50% file: ', train_50_file)
print('Train 25% file: ', train_25_file)

print('Validation file: ', val_file)
print('Test file: ', test_file)

"""Dataset info function to explore the dataset and print different statistics about it and it takes the dataset as parameter and its type to print the information of that dataset"""

def dataset_info(df, type="Train", label="label"):
    """
      It calculates the number and percentage of samples in 
      the DataFrame that have the label "OFF" and "NOT". 
      It then prints information about the DataFrame such as the number of rows,
      the number of columns, and the column names. 
      Finally, it prints a table summarizing the number 
      and percentage of "OFF" and "NOT" samples in the DataFrame.
      
      Args:
        df
        type
        label

      return None
    """
    
    total = len(df)
    off_count = df[df[label] == 'OFF'].shape[0]
    not_count = df[df[label] == 'NOT'].shape[0]
    off_percentage = (off_count / total) * 100
    not_percentage = (not_count / total) * 100
    
    print("Number of rows:", total)
    print("Number of columns:", len(df.columns))
    print("Columns:", df.columns.tolist())
    # print("Data types:")
    # print(df.dtypes)
    # print("Descriptive statistics:")
    # print(df.describe())
    print("----------------------------------------------")
    print("Dataset | Total |  OFF |  NOT | % OFF | % NOT")
    print(type + "   | {:>5} |{:>5} |{:>5} | {:>5.2f} | {:>5.2f}".format(total, off_count, not_count, off_percentage, not_percentage))
    print("----------------------------------------------")

def dataset_head_tail(df, count=5):
  """
    It prints the head and the tail of the dataset
    Args:
      df
      count

    return None
  """
    
  print("-------------------------- Head --------------------------")
  print(df.head(count))
  
  print("-------------------------- Tail --------------------------")
  print(df.tail(count))

def label_ratio_plot(df, label = 'label'):
  """
    plots the ratio between the OFF and NOT label values
    
    Args:
      df
      label

    return None
  """
  
  plt.figure(figsize=(4, 2))
  sns.countplot(x=label, hue=label, data=df, 
              hue_order=['OFF', 'NOT'], 
              palette=['red', 'blue'])
  plt.xlabel('Offensive or Not')
  plt.ylabel('Number of Tweets')
  plt.title('Offensive vs Non-offensive Tweets')
  plt.legend(title=label, loc='upper right')

"""Dataset labels casting maps (Offensive and Not-offensive) to be used later in models as binary values."""

# converting categorical labels to binary values that can be used in ML models
label_mapping = {'OFF': 1, 'NOT': 0}
inverse_label_mapping = {1: 'OFF', 0: 'NOT'}

"""### Datasets statisitics and exploration"""

print("Validation dataset exploration.")
validation_df = pd.read_csv(val_file)
dataset_info(validation_df, type="valid")
validation_df.head()

print("Test dataset exploration.")
test_df = pd.read_csv(test_file)
dataset_info(test_df, type="test")
test_df.head()

print("Train full dataset exploration.")
train_df = pd.read_csv(train_file)
dataset_info(train_df, type="Train")
train_df.tail()

print("Train (25%) dataset exploration.")
train_25_df = pd.read_csv(train_25_file)
label_ratio_plot(train_25_df)
dataset_info(train_25_df, type="Train")
dataset_head_tail(train_25_df)

print("Train (50%) dataset exploration.")
train_50_df = pd.read_csv(train_50_file)
label_ratio_plot(train_50_df)
dataset_info(train_50_df, type="Train")
dataset_head_tail(train_50_df)

print("Train (75%) dataset exploration.")
train_75_df = pd.read_csv(train_75_file)
label_ratio_plot(train_75_df)
dataset_info(train_75_df, type="Train")
dataset_head_tail(train_75_df)

print("Train (100%) dataset exploration.")
train_100_df = pd.read_csv(train_100_file)
label_ratio_plot(train_100_df)
dataset_info(train_100_df, type="Train")
dataset_head_tail(train_100_df)

"""Function to calculate different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation based on y_true and y_pred values."""

def compute_performance(y_true, y_pred, split='test'):
    """
    Prints different performance metrics like Accuracy, 
    Recall (macro), Precision (macro), and F1 (macro).
    This also displays a Confusion Matrix with proper X & Y axis labels.
    Also, returns F1 score.

    Args:
        y_true: numpy array or list
        y_pred: numpy array or list
        split: str

    Returns:
        float
    """

    print('Computing different performance metrics on', split, 'set of Dataset')
    f1score = f1_score(y_true, y_pred, average='macro')
    acc = accuracy_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred, average='macro')
    precision = precision_score(y_true, y_pred, average='macro')
    
    print('F1 Score(macro):', f1score)
    print('Accuracy:', acc)
    print('Recall(macro):', recall)
    print('Precision(macro):', precision)
    print(classification_report(y_true, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5, 3))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=inverse_label_mapping.values(), yticklabels=inverse_label_mapping.values())
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix for ' + split + ' Set')
    plt.show()

    return f1score

def plot_history(history):
    """
    Plots the training and validation loss and accuracy.

    Args:
      - history: history object containing the training history.
    """

    # Plot training & validation accuracy values
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')

    # Plot training & validation loss values
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')

    plt.show()

"""### Data pre preprocessing functions
based on: https://github.com/sondor66/NLP_Offensive_Speech_Exploratory_Analysis/blob/master/OLID_Hate_Speech_Exploratory_Analysis_NLP.ipynb

"""

# function to remove non-ascii characters from text
def remove_non_ascii_chars(text):
    """
    Removes non-ascii characters from the given text

    Args:
    - text (str): Text string to be cleaned

    Return:
    - str: Cleaned text string
    """
    return re.sub(r'[^\x00-\x7F]+', ' ', text)


# function to remove digits from text
def remove_digits(text):
    """
    Removes digits from the given text

    Args:
    - text (str): Text string to be cleaned

    Return:
    - str: Cleaned text string
    """
    return re.sub(r'\d+', '', text)


# function to remove HTML tags from text
def remove_html_tags(text):
    """
    Removes HTML tags from the given text

    Args:
    - text (str): Text string to be cleaned

    Return:
    - str: Cleaned text string
    """
    return re.sub('(?:<[^>]+>)', '', text)


# function to remove multiple spaces from text
def remove_multiple_spaces(text):
    """
    Removes multiple spaces from the given text

    Args:
    - text (str): Text string to be cleaned

    Return:
    - str: Cleaned text string
    """
    return re.sub(r"\s+", " ", text)


# function to remove special characters from text
def remove_special_chars(text):
    """
    Removes special characters from the given text

    Args:
    - text (str): Text string to be cleaned

    Return:
    - str: Cleaned text string
    """
    return re.sub(r"[,@\"'?\.$%_]", "", text, flags=re.I)


# function to convert lowercase except for all caps words
def lowercase_except_all_caps(text):
    """
    Converts all words in the given text to lowercase except
    for the words in all caps

    Args:
    - text (str): Text string to be cleaned.]

    Return:
    - str: Cleaned text string.
    """
    return ' '.join([word if word.isupper() else word.lower() for word in text.split()])


# function to remove stopwords from text
def remove_stopwords(text, stopword_list):
    """
    Removes stopwords from the given text

    Args:
    - text (str): Text string to be cleaned
    - stopword_list (list): List of stopwords to be removed

    Return:
    - str: Cleaned text string
    """
    words = word_tokenize(text)
    return ' '.join([word for word in words if word not in stopword_list])


# function to clean text by applying a series of cleaning steps
def cleaning(intweet):
    """
    Cleans the given text by applying cleaning steps,
    including removing non-ascii characters, digits,
    HTML tags, multiple spaces, special characters, and stopwords

    Args:
    - intweet (str): Text string to be cleaned

    Return:
    - str: Cleaned text string
    """
    clean_tweet = intweet
    clean_tweet = remove_non_ascii_chars(clean_tweet)
    clean_tweet = remove_digits(clean_tweet)
    clean_tweet = remove_html_tags(clean_tweet)
    clean_tweet = remove_multiple_spaces(clean_tweet)
    clean_tweet = remove_special_chars(clean_tweet)
    clean_tweet = lowercase_except_all_caps(clean_tweet)

    stopword_list = set(stopwords.words('english'))
    extra_stopwords = ["\"", "\"", "n't", "'s", "...", "!", "?", "I", "@USER", "USER", "URL", ".", ";", ":", "/", "\\", ",", "#", "@", "$", "&", ")", "(", "\""]
    stopword_list.update(extra_stopwords)

    clean_tweet = remove_stopwords(clean_tweet, stopword_list)
    return clean_tweet

# function to stem words in text
def stem(txt):
    """
    Stems the words in the given text using the Porter stemming algorithm

    Args:
    - txt (str): Text string to be stemmed.

    Return:
    - str: Stemmed text string.
    """
    stemmer = PorterStemmer()
    words = word_tokenize(txt)
    stemmed_words = [stemmer.stem(word) for word in words]
    return ' '.join(stemmed_words)

"""# Method 1 Start

### Model 1 files
"""

# Model 1 directory
MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1')
print('Model 1 directory: ', MODEL_1_DIRECTORY)

# Model 1 trained using 25% of train data directory
MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25')
print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)

# Model 1 trained using 50% of train data directory
MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50')
print('Model 1 directory with 50% data: ', MODEL_1_50_DIRECTORY)

# Model 1 trained using 75% of train data directory
MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75')
print('Model 1 directory with 75% data: ', MODEL_1_75_DIRECTORY)

# Model 1 trained using 100% of train data directory
MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100')
print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)

# Output file using Model 1 trained using 25% of train data
model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv') 
print('Output file name using model 1 using 25% of train data: ', model_1_25_output_test_file)

# Output file using Model 1 trained using 50% of train data
model_1_50_output_test_file = os.path.join(MODEL_1_50_DIRECTORY, 'output_test.csv') 
print('Output file name using model 1 using 50% of train data: ', model_1_50_output_test_file)

# Output file using Model 1 trained using 75% of train data
model_1_75_output_test_file = os.path.join(MODEL_1_75_DIRECTORY, 'output_test.csv') 
print('Output file name using model 1 using 75% of train data: ', model_1_75_output_test_file)

# Output file using Model 1 trained using 100% of train data
model_1_100_output_test_file = os.path.join(MODEL_1_100_DIRECTORY, 'output_test.csv') 
print('Output file name using model 1 using 100% of train data: ', model_1_100_output_test_file)

"""### Model 1 data preprocessing a preperation"""

# function to prepare dataset for training and testing
def prepare_dataset1(data, count_vectorizer=None, split='test'):
  """
  Prepares the dataset for training and testing by 
  applying cleaning and stemming to the text data and converting it 
  into a matrix of token counts using CountVectorizer.

  Args:
  - data : Data Frame object containing the data to be processed.
  - count_vectorizer : An optional pre-fitted CountVectorizer object.
  - split: A string specifying whether to prepare the data for training or testing. 
           Valid values are 'train' and 'test'.

  Return:
    - A tuple containing:
      - X (numpy.ndarray): A matrix of token counts representing the processed text data.
      - count_vectorizer (CountVectorizer): CountVectorizer object that has been fit to the data.
        This object is returned only if split='train'.
  """
  # make a copy of the data
  cleaned_data = data.copy()

  # apply cleaning to the copy
  cleaned_data['tweet'] = cleaned_data['tweet'].apply(cleaning)

  # apply stemming to the copy
  cleaned_data['tweet'] = cleaned_data['tweet'].apply(stem)

  if split == 'train':
      count_vectorizer = CountVectorizer(stop_words='english',max_features=5000) 
      values = count_vectorizer.fit_transform(cleaned_data['tweet'].values) 
  else:
      values = count_vectorizer.transform(cleaned_data['tweet'].values)

  if split == 'train':
      return values, count_vectorizer
  else:
      return values

print(cleaning("@USER #NoPasaran: Unity demo to oppose the far-right"))
print(stem("@USER #NoPasaran: Unity demo to oppose the far-right"))

"""### Model 1 declaration

Create an essemble model that is based on different models like RandomForest, Multinomial Naive Bayes , LogisticsRegression and SVC
"""

def create_ensemble_model():
    """
      Create the esemble model Create an essemble model that is based on 
      different models like RandomForest, LogisticsRegression , 
      Multinomial Naive Bayes and SVC

      Return:
      - ensemble
    """

    print("Creating the ensemble model...")
    clf1 = RandomForestClassifier(n_estimators=100, random_state=student_id)
    clf2 = LogisticRegression(random_state=student_id, max_iter=1000)
    clf3 = SVC(kernel='linear', probability=True, random_state=student_id)
    clf4 = MultinomialNB()
    ensemble = VotingClassifier(estimators=[('rf', clf1), ('lr', clf2), ('svc', clf3), ('mnb', clf4)], voting='soft')
    
    return ensemble

"""Train the esemble model based on x and y trains values"""

def train_ensemble_model(model, X_train, y_train):
    """
      It trains the the esemble model based on x and y train data

      Args:
      - model: the model the needs to be trained
      - X_train: x train data
      - Y_train: y train data

      Return:
      - model
    """
    
    print("Training the ensemble model...")
    model.fit(X_train, y_train)
    return model

def evaluate_model_with_kfold(model, X, y, k=5):
    print("Evaluating the ensemble model using k-fold cross-validation...")
    scores = cross_val_score(model, X, y, cv=k, scoring='accuracy')    
    print("K-fold cross-validation scores:", scores)
    print("Mean accuracy:", scores.mean())
    print("Standard deviation:", scores.std())
    return scores

"""Function that saved the model to the disk based on the model, vectorizer and scaler data and the model directory path and return the saved files."""

def save_model1(model, vectorizer, scaler, model_dir):
    """
      It saves the model, vectorizer, scaler files

      Args:
      - model: the model the needs to be saved
      - vectorizer: vectorizer data
      - scaler: scaler data
      - model_dir: Model output Directory

      Return:
      - model_file: Model file name
      - vectorizer_file: Vectorizer file name
      - scaler_file: Scaler file name
    """

    # save the model to disk
    model_file = os.path.join(model_dir, 'model.sav')
    pickle.dump(model, open(model_file, 'wb'))
    print('Saved model to ', model_file)
    
    # save the vectorizer to disk
    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav') 
    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))
    print('Saved Vectorizer to ', vectorizer_file)

    # save the scaler to disk
    scaler_file = os.path.join(model_dir, 'scaler.sav')
    pickle.dump(scaler, open(scaler_file, 'wb'))
    print('Saved Scaler to ', scaler_file)

    return model_file, vectorizer_file, scaler_file

"""Function that loads the model from the disk based on the model, vectorizer and scaler files and return the loaded files."""

def load_model1(model_file, vectorizer_file, scaler_file):
    """
      It loads the model, vectorizer, scaler files

      Args:
      - model_file: Model file name
      - vectorizer_file: Vectorizer file name
      - scaler_file: Scaler file name

      Return:
      - model: the model the has been loaded
      - vectorizer: vectorizer data
      - scaler: scaler data
    """

    # load model, vectorizer, and scaler from disk
    model = pickle.load(open(model_file, 'rb'))
    print('Loaded model from ', model_file)

    vectorizer = pickle.load(open(vectorizer_file, 'rb'))
    print('Loaded Vectorizer from ', vectorizer_file)

    scaler = pickle.load(open(scaler_file, 'rb'))
    print('Loaded Scaler from ', scaler_file)

    return model, vectorizer, scaler

"""## Training Method 1 Code
Train method take `train_file`, `val_file`,  and `model_dir` as input. It trained on the train_file datapoints, and validate on the val_file datapoints.
"""

def train_method1(train_file, val_file, model_dir):
    """
     Takes train_file, val_file and model_dir as input.
     It trained on the train_file datapoints, and validate on the val_file datapoints.
     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
     After finishing the training, it saved the best model in the model_dir.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory

    Return:
        model_file: Model file name
        vectorizer_file: Vectorizer file name
        scaler_file: Scaler file name

    """
    train_df = pd.read_csv(train_file)
    val_df = pd.read_csv(val_file)

    train_label = train_df['label']
    val_label = val_df['label']

    train_values, count_vectorizer = prepare_dataset1(train_df, split='train') 
    val_values = prepare_dataset1(val_df, count_vectorizer)

    # Scale the data
    scaler = MaxAbsScaler()
    train_values = scaler.fit_transform(train_values)
    val_values = scaler.transform(val_values)

    model = create_ensemble_model()
    model = train_ensemble_model(model, train_values, train_label)

    # k_fold_scores = evaluate_model_with_kfold(model, train_values, train_label, k=5)

    model_file, vectorizer_file, scaler_file = save_model1(model, count_vectorizer, scaler, model_dir)

    train_pred_label = model.predict(train_values)
    val_pred_label = model.predict(val_values)

    print('Train Split')
    train_f1_score = compute_performance(train_label, train_pred_label, split='train')

    print('Validation Split')
    val_f1_score = compute_performance(val_label, val_pred_label, split='valid')

    return model_file, vectorizer_file, scaler_file

"""### Training model 1 on different datasets"""

print('Train model 1 using of 25% of data')
model_25_file, vectorizer_25_file, scaler_25_file = train_method1(train_25_file, val_file, MODEL_1_25_DIRECTORY)

print('Train model 1 using of 50% of data')
model_50_file, vectorizer_50_file, scaler_50_file = train_method1(train_50_file, val_file, MODEL_1_50_DIRECTORY)

print('Train model 1 using of 75% of data')
model_75_file, vectorizer_75_file, scaler_75_file = train_method1(train_75_file, val_file, MODEL_1_75_DIRECTORY)

print('Train model 1 using of 100% of data')
model_100_file, vectorizer_100_file, scaler_100_file = train_method1(train_100_file, val_file, MODEL_1_100_DIRECTORY)

"""## Testing Method 1 Code
The test method for the first model that takes `test_file`, `model_file`, `vectorizer_file`, `scaler_file` and `output_dir` as input. It print all performance metrics, and save the output file in the `output_dir`  
"""

def test_method1(test_file, model_file, output_dir, vectorizer_file, scaler_file):
    """
     take test_file, model_file and output_dir as input.
     It loads model and test of the examples in the test_file.
     It prints different evaluation metrics, and saves the output in output directory

    Args:
        test_file: Test file name
        model_file: Model file name
        output_dir: Output Directory
        vectorizer_file: Vectorizer file name
        scaler_file: Scaler file name

    Return:
      test_df
    
    """

    test_df = pd.read_csv(test_file)
    
    test_label = test_df['label']

    model, vectorizer, scaler = load_model1(model_file, vectorizer_file, scaler_file) 

    test_values = prepare_dataset1(test_df, vectorizer)

    # Scale the test data using the scaler
    test_values = scaler.transform(test_values)

    test_pred_label = model.predict(test_values)

    test_df['out_label'] = test_pred_label

    test_f1_score = compute_performance(test_label, test_pred_label, split='test')

    out_file = os.path.join(output_dir, 'output_test.csv')

    print('Saving model output to', out_file)
    test_df.to_csv(out_file)

    return test_df

"""### Testing Model 1 on different datasets"""

print('Testing model 1 using ensemble model trained on 25% data')
test_25_df = test_method1(test_file, model_25_file, MODEL_1_25_DIRECTORY, vectorizer_25_file, scaler_25_file)
dataset_info(test_25_df, label = "out_label")
dataset_head_tail(test_25_df, 10)

print('Testing model 1 using ensemble model trained on 50% data')
test_50_df = test_method1(test_file, model_50_file, MODEL_1_50_DIRECTORY, vectorizer_50_file, scaler_50_file)
dataset_info(test_50_df, label = "out_label")
dataset_head_tail(test_50_df, 10)

print('Testing using model trained on 75% data')
test_75_df = test_method1(test_file, model_75_file, MODEL_1_75_DIRECTORY, vectorizer_75_file, scaler_75_file)
dataset_info(test_75_df, label = "out_label")
dataset_head_tail(test_75_df, 10)

print('Testing model 1 using ensemble model trained on 100% data')
test_100_df = test_method1(test_file, model_100_file, MODEL_1_100_DIRECTORY, vectorizer_100_file, scaler_100_file)
dataset_info(test_100_df, label = "out_label")
dataset_head_tail(test_100_df, 10)

"""## Method 1 End

# Method 2 Start

### Model 2 files
"""

# Model 2 directory
MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2')
print('Model 2 directory: ', MODEL_2_DIRECTORY)

# Model 2 trained using 25% of train data directory
MODEL_2_25_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25')
print('Model 2 directory with 25% data: ', MODEL_2_25_DIRECTORY)

# Model 2 trained using 50% of train data directory
MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50')
print('Model 2 directory with 50% data: ', MODEL_2_50_DIRECTORY)

# Model 2 trained using 75% of train data directory
MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75')
print('Model 2 directory with 75% data: ', MODEL_2_75_DIRECTORY)

# Model 2 trained using 100% of train data directory
MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100')
print('Model 2 directory with 100% data: ', MODEL_2_100_DIRECTORY)

# Output file using Model 2 trained using 25% of train data
model_2_25_output_test_file = os.path.join(MODEL_2_25_DIRECTORY, 'output_test.csv') 
print('Output file name using model 2 using 25% of train data: ', model_2_25_output_test_file)

# Output file using Model 2 trained using 50% of train data
model_2_50_output_test_file = os.path.join(MODEL_2_50_DIRECTORY, 'output_test.csv') 
print('Output file name using model 2 using 50% of train data: ', model_2_50_output_test_file)

# Output file using Model 2 trained using 75% of train data
model_2_75_output_test_file = os.path.join(MODEL_2_75_DIRECTORY, 'output_test.csv') 
print('Output file name using model 2 using 75% of train data: ', model_2_75_output_test_file)

# Output file using Model 2 trained using 100% of train data
model_2_100_output_test_file = os.path.join(MODEL_2_100_DIRECTORY, 'output_test.csv') 
print('Output file name using model 2 using 100% of train data: ', model_2_100_output_test_file)

"""### Model 2 data preprocessing a preperation

Prepares the dataset for training and testing by applying cleaning to the text data, tokenizing the text data, and padding the sequences to a fixed length.
"""

# function to prepare dataset for training and testing
def prepare_dataset2(data, tokenizer=None, max_length=50, split='test'):
    """
    Prepares the dataset for training and testing by applying 
    cleaning to the text data, tokenizing the text data, and 
    padding the sequences to a fixed length.

    Args:
    - data: Data Frame object containing the data to be processed.
    - tokenizer: An optional pre-fitted Tokenizer object.
    - max_length: specify the maximum length of the padded sequences
    - split: specify whether to prepare the data for training or testing. 
             Valid values are 'train' and 'test'.

    Return:
    - A tuple containing:
        - X: matrix of padded sequences representing the processed text data.
        - tokenizer: Tokenizer object that has been fit to the data.
                     It is returned only if split='train'.
    """

    # copy the data
    cleaned_data = data.copy()

    # apply cleaning to the copy
    cleaned_data['tweet'] = cleaned_data['tweet'].apply(cleaning)

    # apply stemming to the copy
    # cleaned_data['tweet'] = cleaned_data['tweet'].apply(stem)
    
    if split == 'train':
        tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')
        tokenizer.fit_on_texts(cleaned_data['tweet'].values)
        sequences = tokenizer.texts_to_sequences(cleaned_data['tweet'].values)
    else:
        sequences = tokenizer.texts_to_sequences(cleaned_data['tweet'].values)

    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')

    if split == 'train':
        return padded_sequences, tokenizer
    else:
        return padded_sequences

# def create_lstm_model(vocab_size, embedding_dim):
#   """
#   This function creates a deep learning model architecture for sentiment analysis using LSTM units. 
#   The model consists of an embedding layer, an LSTM layer, and two dense layers, with a dropout layer in between.

#   Args:
#   - vocab_size: the size of the vocabulary.
#   - embedding_dim: the dimension of the embedding vectors.

#   Returns:
#   - model: a deep learning model.

#   """
#   model = Sequential()
#   model.add(Embedding(vocab_size, embedding_dim))
#   model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) 
#   model.add(Dense(64, activation='relu'))
#   model.add(Dropout(0.5))
#   model.add(Dense(1, activation='sigmoid'))
#   model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#   return model

"""function to create LSTM model"""

def create_lstm_model(vocab_size, embedding_dim):
    """
    Creates a bidirectional LSTM model for text classification.

    Args:
    - vocab_size: specify the size of the vocabulary.
    - embedding_dim: specify the dimensionality of the embedding.

    Return:
    - model: Keras model object.
    """
    model = Sequential()
    
    # trainable embeddings
    model.add(Embedding(vocab_size, embedding_dim))
    
    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))
    model.add(BatchNormalization())
    
    model.add(Dense(64, activation='relu', kernel_regularizer='l2'))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    
    model.add(Dense(1, activation='sigmoid'))
    
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

# function to train LSTM model
def train_lstm_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):
    """
    Trains the LSTM model on the training data and evaluates it on the validation data.

    Args:
      - model: model object to be trained.
      - X_train: An array of training input data.
      - y_train: An array of training target data.
      - X_val: An array of validation input data.
      - y_val: An array of validation target data.
      - epochs: specifiy the number of training epochs.
      - batch_size: specifiy the batch size for training.

    Return:
      - model: trained Keras model object.
      - history: history object containing the training history.
    """
    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=[early_stopping])
    return model, history

"""Function that saves an LSTM model and tokenizer to files based on the model directory and the model data and tokenizer data."""

# function to save the LSTM model and tokenizer
def save_model2(model, tokenizer, model_dir):
    """
    Saves an LSTM model and tokenizer to files.

    Args:
      - model: Keras model object to be saved.
      - tokenizer: Tokenizer object to be saved.
      - model_dir: string path to the model directory.

    Return:
      - model_file: string path to the saved model file.
      - tokenizer_file : string path to the saved tokenizer file.
    """
    # Save the model to disk
    model_file = os.path.join(model_dir, 'model.h5')
    model.save(model_file)

    print('Saved model 2 to ', model_file)

    # Save the tokenizer to disk using pickle
    tokenizer_file = os.path.join(model_dir, 'tokenizer.sav') 
    pickle.dump(tokenizer, open(tokenizer_file, 'wb'))

    print('Saved tokenizer to ', tokenizer_file)

    # Return the paths to the saved model and tokenizer files
    return model_file, tokenizer_file

# function to load the LSTM model and tokenizer
def load_model2(model_file, tokenizer_file):
    """
    Loads an LSTM model and tokenizer from files.

    Args:
      - model_file: string path to the saved model file.
      - tokenizer_file: string path to the saved tokenizer file.

    Return:
      - model: Keras model object loaded from file.
      - tokenizer: Tokenizer object loaded from file.
    """
    # Load the model from disk
    model = load_model(model_file)

    print('Loaded model 2:', model_file)

    # Load the tokenizer from disk using pickle
    tokenizer = pickle.load(open(tokenizer_file, 'rb'))

    # Return the loaded model and tokenizer as a tuple
    return model, tokenizer

"""## Training Method 2 Code

"""

def train_method2(train_file, val_file, model_dir, epochs=10, embedding_dim=300):
  """
    Takes train_file, val_file and model_dir as input.
    It trained on the train_file datapoints, and validate on the val_file datapoints.
    While training and validating, it print different evaluataion metrics
    and losses, wheverever necessary.
    After finishing the training, it saved the best model in the model_dir.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory
        epochs: specifiy the number of training epochs.
        batch_size: specifiy the batch size for training.
    Return:
        model_file: specify the path to the saved model file.
        tokenizer_file: specify the path to the saved tokenizer file.
  """

  train_df = pd.read_csv(train_file)
  val_df = pd.read_csv(val_file)

  train_df['label'] = train_df['label'].replace(label_mapping)
  val_df['label'] = val_df['label'].replace(label_mapping)

  train_label = train_df['label']
  val_label = val_df['label']

  train_values, tokenizer = prepare_dataset2(train_df, split='train') 
  val_values = prepare_dataset2(val_df, tokenizer)

  vocab_size = len(tokenizer.word_index) + 1

  model = create_lstm_model(vocab_size, embedding_dim)
  model, history = train_lstm_model(model, train_values, train_label, val_values, val_label, epochs)

  model_file, tokenizer_file = save_model2(model, tokenizer, model_dir)

  threshold = 0.5
  train_pred_label = (model.predict(train_values) > threshold)
  val_pred_label = (model.predict(val_values) > threshold)

  train_f1_score = compute_performance(train_label, train_pred_label, split='train')
  val_f1_score = compute_performance(val_label, val_pred_label, split='valid')
  plot_history(history)

  return model_file, tokenizer_file

"""### Training Model 2 on different datasets"""

print('Train model 2 using of 25% of data')
model2_25_file, tokenizer2_25_file = train_method2(train_25_file, val_file, MODEL_2_25_DIRECTORY)

print('Train model 2 using of 50% of data')
model2_50_file, tokenizer2_50_file = train_method2(train_50_file, val_file, MODEL_2_50_DIRECTORY, epochs=10)

print('Train model 2 using of 75% of data')
model2_75_file, tokenizer2_75_file = train_method2(train_75_file, val_file, MODEL_2_75_DIRECTORY, epochs=10)

print('Train model 2 using of 100% of data')
model2_100_file, tokenizer2_100_file = train_method2(train_100_file, val_file, MODEL_2_100_DIRECTORY, epochs=10)

"""## Testing Method 2 Code"""

def test_method2(test_file, model_file, tokenizer_file, output_dir): 
    """
    Takes test_file, model_file, and output_dir as input.
    It loads the model and tests the examples in the test_file.
    It prints different evaluation metrics and saves the output in the output directory.

    Args:
        test_file: Test file name
        model_file: Model file name
        tokenizer_file: Tokenizer file name
        output_dir: Output Directory
    """

    test_df = pd.read_csv(test_file)
    
    test_label = test_df['label']
    test_label = test_label.map(label_mapping)  # Convert labels to integers

    model, tokenizer = load_model2(model_file, tokenizer_file)

    test_values = prepare_dataset2(test_df, tokenizer)

    threshold = 0.5
    test_pred_label = (model.predict(test_values) > threshold)  # Convert predictions to integers

    # Use numpy.vectorize to apply the mapping function to the predicted labels
    map_labels = np.vectorize(inverse_label_mapping.get)
    test_df['out_label'] = map_labels(test_pred_label[:, 0])  # Convert integers back to labels

    test_f1_score = compute_performance(test_label, test_pred_label, split='test')

    out_file = os.path.join(output_dir, 'output_test.csv')

    print('Saving model output to', out_file)
    test_df.to_csv(out_file)

    return test_df

"""### Testing Model 2 on different datasets"""

print('Testing using model 2 trained on 25% data')
test2_25_df = test_method2(test_file, model2_25_file, tokenizer2_25_file, MODEL_2_25_DIRECTORY)
dataset_info(test2_25_df, label = "out_label")
dataset_head_tail(test2_25_df, 10)

print('Testing using model 2 trained on 50% data')
test2_50_df = test_method2(test_file, model2_50_file, tokenizer2_50_file, MODEL_2_50_DIRECTORY)
dataset_info(test2_50_df, label = "out_label")
dataset_head_tail(test2_50_df, 10)

print('Testing using model 2 trained on 75% data')
test2_75_df = test_method2(test_file, model2_75_file, tokenizer2_75_file, MODEL_2_75_DIRECTORY)
dataset_info(test2_75_df, label = "out_label")
dataset_head_tail(test2_75_df, 10)

print('Testing using model 2 trained on 100% data')
test2_100_df = test_method2(test_file, model2_100_file, tokenizer2_100_file, MODEL_2_100_DIRECTORY)
dataset_info(test2_100_df, type = "test", label = "out_label")
dataset_head_tail(test2_100_df, 25)

"""## Method 2 End

Models comparison
"""

data_sizes = [25, 50, 75, 100]
ensemble_val_accuracies = [0.74, 0.75, 0.75, 0.76]
ensemble_test_accuracies = [0.8, 0.79, 0.81, 0.81]
lstm_val_accuracies = [0.72, 0.74, 0.71, 0.72]
lstm_test_accuracies = [0.74, 0.75, 0.76, 0.75]

plt.figure(figsize=(10, 6))

plt.plot(data_sizes, ensemble_val_accuracies, label='Ensemble Validation', linestyle='--', marker='o')
plt.plot(data_sizes, ensemble_test_accuracies, label='Ensemble Test', marker='o')
plt.plot(data_sizes, lstm_val_accuracies, label='LSTM Validation', linestyle='--', marker='o')
plt.plot(data_sizes, lstm_test_accuracies, label='LSTM Test', marker='o')

plt.xlabel('Fraction of Data %')
plt.ylabel('Accuracy')
plt.title('Performance on Different Data Sizes')
plt.legend(loc='lower right')
plt.grid()
# plt.ylim(0) 

plt.show()

"""# Other Method/model Start"""



"""##Other Method/model End"""